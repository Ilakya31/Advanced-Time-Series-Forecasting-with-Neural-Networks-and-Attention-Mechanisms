{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588177ee-1d43-4a79-ab02-1a727bb0129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# advanced_ts_attention_forecasting.py\n",
    "# Single-file implementation:\n",
    "# - loads AirPassengers (statsmodels) or generates synthetic fallback\n",
    "# - preprocessing (scaling, windowing)\n",
    "# - custom additive temporal attention layer\n",
    "# - Attention-LSTM and baseline LSTM models (Keras)\n",
    "# - rolling-origin evaluation and SARIMA baseline\n",
    "# - plotting attention heatmap and saving a small text report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, backend as K, Input, Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import math\n",
    "import time\n",
    "\n",
    "# 1) Load AirPassengers\n",
    "try:\n",
    "    dataset = sm.datasets.get_rdataset(\"AirPassengers\").data\n",
    "    if 'value' in dataset.columns:\n",
    "        ts = dataset['value'].astype(float)\n",
    "    else:\n",
    "        ap = sm.datasets.airpassengers.load_pandas().data\n",
    "        if 'AirPassengers' in ap.columns:\n",
    "            ts = ap['AirPassengers'].astype(float)\n",
    "        else:\n",
    "            ts = ap.iloc[:,0].astype(float)\n",
    "except Exception as e:\n",
    "    print(\"Could not load AirPassengers; generating synthetic demo series. Error:\", e)\n",
    "    rng = pd.date_range(\"1949-01-01\", periods=144, freq='M')\n",
    "    ts = pd.Series(100 + 0.5*np.arange(144) + 10*np.sin(np.arange(144)/12*2*np.pi) + np.random.randn(144)*5, index=rng)\n",
    "\n",
    "# ensure monthly datetime index if not present\n",
    "if not isinstance(ts, pd.Series):\n",
    "    ts = pd.Series(ts)\n",
    "if ts.index.dtype == 'int64' or not hasattr(ts.index, 'freq'):\n",
    "    ts.index = pd.date_range(\"1949-01-01\", periods=len(ts), freq='M')\n",
    "\n",
    "print(f\"Loaded series length: {len(ts)}; start: {ts.index[0].date()}\")\n",
    "\n",
    "# 2) Preprocessing helper\n",
    "def create_windows(series, input_len, output_len, stride=1):\n",
    "    X, y = [], []\n",
    "    L = len(series)\n",
    "    for start in range(0, L - input_len - output_len + 1, stride):\n",
    "        end = start + input_len\n",
    "        X.append(series[start:end])\n",
    "        y.append(series[end:end+output_len])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X[..., np.newaxis], y  # add feature dim\n",
    "\n",
    "# 3) Custom additive temporal attention layer (Keras)\n",
    "class TemporalSelfAttention(Layer):\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.return_attention = return_attention\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.timesteps = input_shape[1]\n",
    "        self.features = input_shape[2]\n",
    "        self.W = self.add_weight(shape=(self.features, self.features),\n",
    "                                 initializer='glorot_uniform', name='W_att')\n",
    "        self.b = self.add_weight(shape=(self.features,), initializer='zeros', name='b_att')\n",
    "        self.v = self.add_weight(shape=(self.features,), initializer='glorot_uniform', name='v_att')\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs: (batch, T, F)\n",
    "        score = K.tanh(K.dot(inputs, self.W) + self.b)          # (batch, T, F)\n",
    "        score = K.dot(score, K.expand_dims(self.v))            # (batch, T, 1)\n",
    "        score = K.squeeze(score, axis=-1)                      # (batch, T)\n",
    "        attention_weights = K.softmax(score, axis=-1)          # (batch, T)\n",
    "        attention_weights_expanded = K.expand_dims(attention_weights, axis=-1)\n",
    "        weighted_seq = inputs * attention_weights_expanded     # (batch, T, F)\n",
    "        if self.return_attention:\n",
    "            return weighted_seq, attention_weights\n",
    "        return weighted_seq\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\"return_attention\": self.return_attention})\n",
    "        return config\n",
    "\n",
    "# 4) Model builders\n",
    "def build_attention_lstm(input_len, output_len, n_features=1, lstm_units=64, dense_units=32, learning_rate=1e-3):\n",
    "    inputs = Input(shape=(input_len, n_features))\n",
    "    x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True))(inputs)\n",
    "    att_layer = TemporalSelfAttention(return_attention=False)\n",
    "    weighted_seq = att_layer(x)\n",
    "    context = layers.GlobalAveragePooling1D()(weighted_seq)\n",
    "    x = layers.Dense(dense_units, activation='relu')(context)\n",
    "    outputs = layers.Dense(output_len)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    model._att_layer = att_layer\n",
    "    return model\n",
    "\n",
    "def build_baseline_lstm(input_len, output_len, n_features=1, lstm_units=64, dense_units=32, learning_rate=1e-3):\n",
    "    inputs = Input(shape=(input_len, n_features))\n",
    "    x = layers.LSTM(lstm_units)(inputs)\n",
    "    x = layers.Dense(dense_units, activation='relu')(x)\n",
    "    outputs = layers.Dense(output_len)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    return model\n",
    "\n",
    "# 5) Rolling-origin evaluation routine (fits model on expanding window and forecasts horizon)\n",
    "def rolling_origin_forecast(series, input_len, output_len, model_builder, scaler=None, initial_train_size=100, max_splits=5, fit_kwargs=None, **model_kwargs):\n",
    "    n = len(series)\n",
    "    metrics = []\n",
    "    models = []\n",
    "    idx = initial_train_size\n",
    "    splits = 0\n",
    "    while idx + output_len <= n and splits < max_splits:\n",
    "        train = series[:idx]\n",
    "        test = series[idx: idx+output_len]\n",
    "        local_scaler = MinMaxScaler(feature_range=(0,1)) if scaler is None else scaler\n",
    "        train_scaled = local_scaler.fit_transform(train.reshape(-1,1)).flatten()\n",
    "        X_train, y_train = create_windows(train_scaled, input_len, output_len)\n",
    "        model = model_builder(input_len, output_len, **model_kwargs)\n",
    "        es = EarlyStopping(patience=10, restore_best_weights=True, monitor='loss', verbose=0)\n",
    "        model.fit(X_train, y_train, epochs=50, batch_size=16, callbacks=[es], verbose=0, **(fit_kwargs or {}))\n",
    "        full_scaled = local_scaler.transform(series.reshape(-1,1)).flatten()\n",
    "        last_input = full_scaled[idx-input_len:idx].reshape(1, input_len, 1)\n",
    "        y_pred_scaled = model.predict(last_input).flatten()\n",
    "        y_pred = local_scaler.inverse_transform(y_pred_scaled.reshape(-1,1)).flatten()\n",
    "        rmse = math.sqrt(mean_squared_error(test, y_pred))\n",
    "        mae = mean_absolute_error(test, y_pred)\n",
    "        mape = np.mean(np.abs((test - y_pred) / test)) * 100\n",
    "        metrics.append({'split_start': idx-input_len, 'train_end': idx-1, 'rmse': rmse, 'mae': mae, 'mape': mape})\n",
    "        models.append((model, local_scaler))\n",
    "        idx += output_len\n",
    "        splits += 1\n",
    "    return metrics, models\n",
    "\n",
    "# 6) SARIMA rolling benchmark\n",
    "def sarima_rolling(series, order=(1,1,1), seasonal_order=(1,1,1,12), initial_train=84, output_len=12, max_splits=3):\n",
    "    n = len(series)\n",
    "    idx = initial_train\n",
    "    metrics = []\n",
    "    models = []\n",
    "    splits = 0\n",
    "    while idx + output_len <= n and splits < max_splits:\n",
    "        train = series[:idx]\n",
    "        test = series[idx: idx+output_len]\n",
    "        try:\n",
    "            mod = SARIMAX(train, order=order, seasonal_order=seasonal_order, enforce_stationarity=False, enforce_invertibility=False)\n",
    "            res = mod.fit(disp=False)\n",
    "            pred = res.get_forecast(steps=output_len).predicted_mean\n",
    "            rmse = math.sqrt(mean_squared_error(test, pred))\n",
    "            mae = mean_absolute_error(test, pred)\n",
    "            mape = np.mean(np.abs((test - pred) / test)) * 100\n",
    "        except Exception as e:\n",
    "            print(\"SARIMA fit failed at idx\", idx, \"error:\", e)\n",
    "            rmse = mae = mape = np.nan\n",
    "            res = None\n",
    "        metrics.append({'split_start': idx-output_len, 'train_end': idx-1, 'rmse': rmse, 'mae': mae, 'mape': mape})\n",
    "        models.append(res)\n",
    "        idx += output_len\n",
    "        splits += 1\n",
    "    return metrics, models\n",
    "\n",
    "# 7) Experiment config (change these for longer/stronger runs)\n",
    "series_values = ts.values.astype(float)\n",
    "INPUT_LEN = 24\n",
    "OUTPUT_LEN = 12\n",
    "INITIAL_TRAIN = 84\n",
    "MAX_SPLITS = 3\n",
    "\n",
    "# Demo hyperparams (small grid)\n",
    "chosen_hparams = {\n",
    "    'lstm_units': 32,\n",
    "    'dense_units': 16,\n",
    "    'learning_rate': 1e-3\n",
    "}\n",
    "\n",
    "print(\"Starting rolling-origin evaluations (demo config). This may take several minutes depending on your machine.\")\n",
    "\n",
    "start = time.time()\n",
    "att_metrics, att_models = rolling_origin_forecast(series_values, INPUT_LEN, OUTPUT_LEN, build_attention_lstm,\n",
    "                                                  initial_train_size=INITIAL_TRAIN, max_splits=MAX_SPLITS,\n",
    "                                                  fit_kwargs={'verbose':0}, **chosen_hparams)\n",
    "base_metrics, base_models = rolling_origin_forecast(series_values, INPUT_LEN, OUTPUT_LEN, build_baseline_lstm,\n",
    "                                                    initial_train_size=INITIAL_TRAIN, max_splits=MAX_SPLITS,\n",
    "                                                    fit_kwargs={'verbose':0}, **chosen_hparams)\n",
    "sarima_metrics, sarima_models = sarima_rolling(series_values, order=(2,1,2), seasonal_order=(1,1,1,12),\n",
    "                                              initial_train=INITIAL_TRAIN, output_len=OUTPUT_LEN, max_splits=MAX_SPLITS)\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"Done. Time elapsed: {elapsed:.1f}s\")\n",
    "print(\"Attention-LSTM metrics per split:\")\n",
    "for m in att_metrics:\n",
    "    print(m)\n",
    "print(\"Baseline LSTM metrics per split:\")\n",
    "for m in base_metrics:\n",
    "    print(m)\n",
    "print(\"SARIMA metrics per split:\")\n",
    "for m in sarima_metrics:\n",
    "    print(m)\n",
    "\n",
    "# 8) Aggregate metrics\n",
    "def aggregate_metrics(metrics_list):\n",
    "    df = pd.DataFrame(metrics_list)\n",
    "    return {'rmse_mean': df['rmse'].mean(), 'mae_mean': df['mae'].mean(), 'mape_mean': df['mape'].mean()}\n",
    "\n",
    "agg_att = aggregate_metrics(att_metrics)\n",
    "agg_base = aggregate_metrics(base_metrics)\n",
    "agg_sarima = aggregate_metrics(sarima_metrics)\n",
    "results_table = pd.DataFrame([{'model':'Attention-LSTM', **agg_att},\n",
    "                              {'model':'Baseline LSTM', **agg_base},\n",
    "                              {'model':'SARIMA', **agg_sarima}]).set_index('model')\n",
    "print(\"\\nAggregated metrics (mean across splits):\")\n",
    "print(results_table.round(3))\n",
    "\n",
    "# 9) Attention extraction & visualization (for last fitted Attention-LSTM)\n",
    "if len(att_models) > 0:\n",
    "    sample_model, sample_scaler = att_models[-1]\n",
    "    # find encoder (Bidirectional LSTM) layer\n",
    "    encoder_layer = None\n",
    "    for layer in sample_model.layers:\n",
    "        if isinstance(layer, layers.Bidirectional):\n",
    "            encoder_layer = layer\n",
    "            break\n",
    "    if encoder_layer is not None and hasattr(sample_model, '_att_layer'):\n",
    "        sub = Model(sample_model.input, encoder_layer.output)\n",
    "        # prepare last input\n",
    "        train_end_idx = att_metrics[-1]['train_end'] + 1\n",
    "        scaled_full = sample_scaler.transform(series_values.reshape(-1,1)).flatten()\n",
    "        last_input = scaled_full[train_end_idx-INPUT_LEN:train_end_idx].reshape(1, INPUT_LEN, 1)\n",
    "        enc_out = sub.predict(last_input)  # (1, T, F)\n",
    "        # recreate attention layer with return_attention=True and copy weights\n",
    "        att_recreate = TemporalSelfAttention(return_attention=True)\n",
    "        _ = att_recreate(enc_out)\n",
    "        att_recreate.set_weights(sample_model._att_layer.get_weights())\n",
    "        weighted_seq, att_w = att_recreate(enc_out)\n",
    "        att_w = att_w.numpy()  # (1, T)\n",
    "        # plot\n",
    "        plt.figure(figsize=(9,2))\n",
    "        plt.imshow(att_w, aspect='auto')\n",
    "        plt.title('Attention weights (last split sample)')\n",
    "        plt.xlabel('time step (older -> newer)')\n",
    "        plt.colorbar(label='weight')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Could not find encoder/attention layer for visualization.\")\n",
    "\n",
    "# 10) Save a small textual report\n",
    "report_lines = []\n",
    "report_lines.append(\"Advanced Time Series Forecasting â€” Experiment Report (demo)\\n\")\n",
    "report_lines.append(\"Dataset: AirPassengers (or synthetic fallback)\\n\\n\")\n",
    "report_lines.append(\"Aggregated metrics (mean across splitting):\\n\")\n",
    "report_lines.append(results_table.to_csv() + \"\\n\")\n",
    "report_lines.append(\"Chosen hyperparameters:\\n\")\n",
    "for k,v in chosen_hparams.items():\n",
    "    report_lines.append(f\"{k}: {v}\\n\")\n",
    "report_lines.append(\"\\nNotes on interpretability: TemporalSelfAttention is additive attention across encoder timesteps; the heatmap above highlights which historical months the model emphasized.\\n\")\n",
    "with open(\"ts_forecasting_report.txt\",\"w\") as f:\n",
    "    f.writelines(report_lines)\n",
    "print(\"Report saved to ts_forecasting_report.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
